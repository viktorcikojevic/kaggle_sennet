defaults:
  - _self_
  - model: unet3d
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe

val_check_interval: 5000
batch_size: 32 # final batch size.
apparent_batch_size: 2 # batch size during training
num_workers: 16
dry_logger: false
exp_name: three_d_seg
group_name: null # maybe this would be useful for grouping experiments when we do hyperparameter search
patience: 5 # stops if (val_check_interval * patience) steps have passed without improvement in the metric
max_epochs: 100

optimiser:
  type: "AdamW"
  log_lr: -3 # sets lr = 10^{-log_lr}. If ls is not null, then it is used instead of log_lr
  kwargs:
    lr: null

loss:
  - type: BCELoss
    weight: 1.0
#  - type: DiceLoss
#    weight: 1.0
#    kwargs:
#      smooth: 0.0001
#  - type: FocalLoss
#    weight: 1.0
#    kwargs:
#      gamma: 2.0

task:
  kwargs:
    eval_threshold: 0.2

train_folders:
  - kidney_1_dense
#  - kidney_3_dense

val_folders:
#  - kidney_1_dense
   - kidney_3_dense
#   - kidney_3_sparse

dataset:
  train_substride: 0.25
  val_substride: 1.0
  kwargs:
    crop_size: 128
    n_take_channels: 16
    reduce_zero_label: False
    assert_label_exists: True
    channel_start: 0
    channel_end: null
    sample_with_mask: false

    crop_size_range: null
    output_crop_size: null
    to_float32: true
    channels_jitter: 0
    p_channel_jitter: 0.0
    load_ann: true
    seg_fill_val: 0
    crop_location_noise: 0

    # These three allow for rotation and flip augmentation
    add_depth_along_channel: true
    add_depth_along_width: true
    add_depth_along_height: true



    normalisation_kwargs:
      mean: 0.5
      std: 0.235
      normalisation_percentile: null


augmentations:
  p_any_augm: 0.5
  random_crop: true # random_crop augmentation
  random_3d_rotate: true # random_3d_rotate augmentation
  random_brightness_contrast:
    p: 0.5
    brightness_limit: 0.2
    contrast_limit: 0.2
  affine:
    scale: [0.8, 1.2]
    translate_percent: [0.1, 0.1]
    p: 0.25
  channel_dropout:
    channel_drop_range: [1, 4]
    p: 0.2
  one_of:
    gaussian_blur: {}
    motion_blur: {}
    p: 0.1
  pixel_dropout:
    per_channel: true
    p: 0.1
  channel_inversion:
    p: 0.1
  zoom_in_out: # random zoom in/out. Guess this is important. Train data resolution ~50um, private test 63um (factor of 1.26)
    scale_limit: 0.4 # If scale_limit is a single float value, the range will be (-scale_limit, scale_limit). Note that the scale_limit will be biased by 1. If scale_limit is a tuple, like (low, high), sampling will be done from the range (1 + low, 1 + high). Default: (-0.1, 0.1).
    p: 0.4

batch_transform:
  kwargs:
    alpha_cutmix: 0.4
    alpha_mixup: 0.4
    cutmix_prob: 0.5
    mixup_prob: 0.5


hydra:
  sweeper:
    sampler:
      seed: 42
    direction: maximize
    storage: null
    n_trials: 20
    n_jobs: 1
    params:
      optimiser.log_lr: range(-4, -3, 1)
