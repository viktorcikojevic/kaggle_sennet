defaults:
  - _self_
  - model: smp_unet_r50
  - augmentation: vanilla
#  - augmentation: noop
  - override hydra/sweeper: optuna
  - override hydra/sweeper/sampler: tpe

val_check_interval: 10000
batch_size: 32 # final batch size.
apparent_batch_size: 32 # batch size during training
num_workers: 16
dry_logger: false
quit_immediately: false # just prints out the config and exits, useful for debugging
exp_name: three_d_seg
group_name: "smp_unet_r50_hyptun" # maybe this would be useful for grouping experiments when we do hyperparameter search
patience: 5 # stops if (val_check_interval * patience) steps have passed without improvement in the metric
max_epochs: 100

optimiser:
  type: "AdamW"
  log_lr: -3.7 # sets lr = 10^{-log_lr}. If ls is not null, then it is used instead of log_lr
  kwargs:
    lr: null

loss:
  - type: BCELoss
    weight: 1.0
  - type: MccLoss
    weight: 1.0
  - type: DiceLoss
    weight: 1.0
    kwargs:
      smooth: 0.0001
#  - type: FocalLoss
#    weight: 1.0
#    kwargs:
#      gamma: 2.0

task:
  kwargs:
    eval_threshold: 0.2
    compute_crude_metrics: false
    ema_momentum: 0.00025

train_folders:
  - kidney_1_dense
#  - kidney_2
#  - kidney_3_dense

val_folders:
#  - kidney_1_dense
   - kidney_3_dense
#   - kidney_3_sparse

batch_transform:
  kwargs:
    alpha_cutmix: 0.4
    alpha_mixup: 0.4
    cutmix_prob: 0.0
    mixup_prob: 0.0

dataset:
  train_substride: 0.25
  val_substride: 1.0
  kwargs:
    crop_size: 512
    n_take_channels: 1
    reduce_zero_label: False
    assert_label_exists: True
    channel_start: 0
    channel_end: null
    sample_with_mask: false

    crop_size_range: [400, 600]
#    crop_size_range: null
    p_crop_size_noise: 0.25
    p_crop_size_keep_ar: 0.5
    output_crop_size: null
    to_float32: true
    load_ann: true
    seg_fill_val: 0
    crop_location_noise: 50
    p_crop_location_noise: 1.0

    # These three allow for rotation and flip augmentation
    add_depth_along_channel: true
    add_depth_along_width: true
    add_depth_along_height: true

    normalisation_kwargs:
      mean: 0.5
      std: 0.235
      normalisation_percentile: 1

hydra:
  sweeper:
    sampler:
      seed: 42
    direction: maximize
    storage: null
    n_trials: 40
    n_jobs: 1
    params:
      optimiser.log_lr: range(-5, -3, 0.1)
#      dataset.kwargs.crop_size: range(128, 992, 32)
#      dataset.kwargs.p_crop_size_noise: range(0.2, 1.0, 0.2)
#      batch_size: range(4, 128, 4)
#
#      batch_transform.kwargs.alpha_cutmix: range(0.0, 1.0, 0.1)
#      batch_transform.kwargs.alpha_mixup: range(0.0, 1.0, 0.1)
#      batch_transform.kwargs.cutmix_prob: range(0.0, 1.0, 0.1)
#      batch_transform.kwargs.mixup_prob: range(0.0, 1.0, 0.1)
      loss.1.weight: range(0.0, 1.0, 0.1)
      loss.2.weight: range(0.0, 1.0, 0.1)
      loss.3.weight: range(0.0, 1.0, 0.1)
      augmentation.augmenter_kwargs.p_any_augm: range(0.0, 1.0, 0.1)
